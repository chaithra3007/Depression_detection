{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= np.load(\"/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Features/test/new_test_audio_feats.npz\", allow_pickle=True)['audio_features']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for i,audio in enumerate(b):\n",
    "    if len(np.array(audio).shape)==3:\n",
    "        continue\n",
    "    else:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the labels\n",
    "test_labels = np.load(\"/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Features/test/test_c_labels.npz\", allow_pickle=True)['labels']\n",
    "\n",
    "# Remove labels at indices 24 and 25\n",
    "indices_to_remove = [24, 25]\n",
    "test_labels = np.delete(test_labels, indices_to_remove, axis=0)\n",
    "\n",
    "# Print the updated length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.load(\"/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Features/train/train_audio_feats.npz\", allow_pickle=True)['audio_features']\n",
    "b= np.load(\"/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Features/test/test_audio_feats.npz\", allow_pickle=True)['audio_features']\n",
    "\n",
    "t1=[np.squeeze(np.array(audio), axis=1) for audio in a]\n",
    "t2 = [\n",
    "    np.squeeze(np.array(audio), axis=1)\n",
    "    for audio in b\n",
    "    if len(np.array(audio).shape) == 3\n",
    "]\n",
    "\n",
    "train_audio_features=t1\n",
    "test_audio_features=t2\n",
    "train_labels = np.load(\"/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Features/train/train_c_labels.npz\", allow_pickle=True)['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104, 69, 256])\n",
      "torch.Size([30, 63, 256])\n"
     ]
    }
   ],
   "source": [
    "train_audio_features_tensors = [torch.tensor(f) for f in train_audio_features]\n",
    "# Now pad the sequences to ensure uniform length\n",
    "padded_train_audio_features = pad_sequence(train_audio_features_tensors, batch_first=True)\n",
    "\n",
    "# Print the shape of the padded tensor\n",
    "print(padded_train_audio_features.shape)\n",
    "\n",
    "test_audio_features_tensors = [torch.tensor(f) for f in test_audio_features]\n",
    "padded_test_audio_features = pad_sequence(test_audio_features_tensors, batch_first=True)\n",
    "print(padded_test_audio_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels, dtype=np.int64)\n",
    "test_labels = np.array(test_labels, dtype=np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104, 17664])\n",
      "torch.Size([148, 69, 256])\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "labels_t = np.squeeze(train_labels)  # Converts to 1D if it's in shape (n_samples, 1)\n",
    "\n",
    "# Flatten the audio features to 2D (n_samples, sequence_length * num_features)\n",
    "flattened_train_audio_features = padded_train_audio_features.view(padded_train_audio_features.size(0), -1)\n",
    "print(flattened_train_audio_features.shape)  # Should be [n_samples, 63 * 256]\n",
    "\n",
    "# Apply SMOTE to balance the data\n",
    "smote = SMOTE(random_state=42)\n",
    "balanced_features, balanced_labels = smote.fit_resample(flattened_train_audio_features.numpy(), labels_t)\n",
    "\n",
    "# Convert the balanced features and labels to PyTorch tensors\n",
    "balanced_features = torch.tensor(balanced_features).to(device)\n",
    "balanced_labels = torch.tensor(balanced_labels).to(device)\n",
    "# Check the current total size and calculate time steps\n",
    "num_samples = balanced_features.size(0)\n",
    "num_features = 256  # Assuming this is fixed\n",
    "time_steps = balanced_features.size(1) // num_features\n",
    "\n",
    "# Reshape to 3D\n",
    "balanced_features = balanced_features.view(num_samples, time_steps, num_features)\n",
    "print(balanced_features.shape)  # Verify the shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_audio_features and test_audio_features have been padded earlier\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_features, labels):\n",
    "        self.audio_features = audio_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # No need to wrap the features and labels again with torch.tensor\n",
    "        return self.audio_features[idx], torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = AudioDataset(padded_train_audio_features, train_labels)\n",
    "test_dataset = AudioDataset(padded_test_audio_features, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming train_audio_features and test_audio_features have been padded earlier\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_features, labels):\n",
    "        self.audio_features = audio_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # No need to wrap the features and labels again with torch.tensor\n",
    "        return self.audio_features[idx], torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = AudioDataset(balanced_features, balanced_labels)\n",
    "test_dataset = AudioDataset(padded_test_audio_features, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioBiLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(AudioBiLSTM, self).__init__()\n",
    "        self.num_classes = config['num_classes']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.dropout = config['dropout']\n",
    "        self.hidden_dims = config['hidden_dims']\n",
    "        self.rnn_layers = config['rnn_layers']\n",
    "        self.embedding_size = config['embedding_size']\n",
    "        self.bidirectional = config['bidirectional']\n",
    "\n",
    "        self.build_model()\n",
    "        # self.init_weight()\n",
    "\n",
    "    def init_weight(net):\n",
    "        for name, param in net.named_parameters():\n",
    "            if not 'ln' in name:\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0.0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def build_model(self):\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.lstm_net_audio = nn.GRU(self.embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.dropout, batch_first=True)\n",
    "\n",
    "        self.ln = nn.LayerNorm(self.embedding_size)\n",
    "\n",
    "        # FC\n",
    "        self.fc_audio = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes),\n",
    "          \n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] \n",
    "        #         h = lstm_out\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "       # print(atten_w.shape, m.transpose(1, 2).shape)\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln(x)\n",
    "        x, _ = self.lstm_net_audio(x)\n",
    "        x = x.mean(dim=1)\n",
    "        out = self.fc_audio(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_classes': 2,\n",
    "    'dropout': 0.4,\n",
    "    #0.5\n",
    "    'rnn_layers': 4,\n",
    "    #2\n",
    "    'embedding_size': 256,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 6e-4,\n",
    "    'hidden_dims': 256,\n",
    "    'bidirectional': False,\n",
    "    'cuda': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioBiLSTM(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train(epoch, train_loader,val_loader,best_accuracy):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Get sequence lengths (for packing)\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        target = target.squeeze() \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / len(train_loader.dataset)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Train Epoch: {epoch}, Loss: {avg_train_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), '/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Classification/Models/audio/audio_best_model_5.pth')  # Save the best model\n",
    "    return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69106/4174753997.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return self.audio_features[idx], torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1, Loss: 0.6915, Accuracy: 0.4595\n",
      "Validation Loss: 0.7027, Validation Accuracy: 0.3667\n",
      "Best model saved with accuracy: 0.3667\n",
      "Train Epoch: 2, Loss: 0.5006, Accuracy: 0.8514\n",
      "Validation Loss: 0.8816, Validation Accuracy: 0.5667\n",
      "Best model saved with accuracy: 0.5667\n",
      "Train Epoch: 3, Loss: 0.2201, Accuracy: 0.9595\n",
      "Validation Loss: 2.1828, Validation Accuracy: 0.6000\n",
      "Best model saved with accuracy: 0.6000\n",
      "Train Epoch: 4, Loss: 0.0686, Accuracy: 0.9730\n",
      "Validation Loss: 2.2497, Validation Accuracy: 0.4000\n",
      "Best model saved with accuracy: 0.6000\n",
      "Train Epoch: 5, Loss: 0.0895, Accuracy: 0.9730\n",
      "Validation Loss: 1.5651, Validation Accuracy: 0.5333\n",
      "Best model saved with accuracy: 0.6000\n",
      "Train Epoch: 6, Loss: 0.0315, Accuracy: 0.9865\n",
      "Validation Loss: 1.6471, Validation Accuracy: 0.6667\n",
      "Best model saved with accuracy: 0.6667\n",
      "Train Epoch: 7, Loss: 0.0078, Accuracy: 1.0000\n",
      "Validation Loss: 1.9473, Validation Accuracy: 0.6000\n",
      "Best model saved with accuracy: 0.6667\n",
      "Train Epoch: 8, Loss: 0.0012, Accuracy: 1.0000\n",
      "Validation Loss: 2.0778, Validation Accuracy: 0.6333\n",
      "Best model saved with accuracy: 0.6667\n",
      "Train Epoch: 9, Loss: 0.0006, Accuracy: 1.0000\n",
      "Validation Loss: 2.2342, Validation Accuracy: 0.6000\n",
      "Best model saved with accuracy: 0.6667\n",
      "Train Epoch: 10, Loss: 0.0003, Accuracy: 1.0000\n",
      "Validation Loss: 2.3176, Validation Accuracy: 0.6000\n",
      "Best model saved with accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, config['epochs'] + 1):\n",
    "    best_accuracy=train(epoch, train_loader,test_loader,best_accuracy)\n",
    "    print(f\"Best model saved with accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_loss = 0.0  \n",
    "    total=0\n",
    "    all_preds = []  # To store all predictions\n",
    "    all_targets = []  # To store all true labels\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            target = target.squeeze()\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "            \n",
    "        accuracy = correct / total\n",
    "        average_loss = total_loss / len(test_loader)\n",
    "        results_df = pd.DataFrame({\n",
    "        'Predicted': all_preds,\n",
    "        'Target': all_targets\n",
    "        })\n",
    "    \n",
    "        #print(\"\\nPredicted vs Target:\")\n",
    "        #print(results_df)\n",
    "        \n",
    "        f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "        recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "        \n",
    "    print(f\"Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted vs Target:\n",
      "    Predicted  Target\n",
      "0           0       0\n",
      "1           0       0\n",
      "2           0       0\n",
      "3           0       1\n",
      "4           0       1\n",
      "5           1       1\n",
      "6           0       1\n",
      "7           1       1\n",
      "8           0       0\n",
      "9           0       1\n",
      "10          1       1\n",
      "11          0       0\n",
      "12          0       0\n",
      "13          1       0\n",
      "14          0       0\n",
      "15          0       0\n",
      "16          1       1\n",
      "17          1       0\n",
      "18          0       1\n",
      "19          0       0\n",
      "20          0       1\n",
      "21          1       0\n",
      "22          0       0\n",
      "23          0       1\n",
      "24          0       0\n",
      "25          0       0\n",
      "26          0       0\n",
      "27          0       0\n",
      "28          1       1\n",
      "29          0       0\n",
      "Loss: 1.6471, Accuracy: 0.6667\n",
      "F1 Score: 0.6500, Precision: 0.6591, Recall: 0.6667\n",
      "Confusion Matrix:\n",
      "[[15  3]\n",
      " [ 7  5]]\n"
     ]
    }
   ],
   "source": [
    "############# DATA Aug  #################\n",
    "\n",
    "model.load_state_dict(torch.load('/mnt/sd1/jhansi/interns/chaithra/MS/sal_project/Classification/Models/audio/audio_best_model_5.pth', weights_only=True))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Evaluate on the test set\n",
    "accuracy = evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
